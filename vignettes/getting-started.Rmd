---
title: "The LMTP Package"
author: "Nick Williams & Ivan Diaz"
output:
  rmarkdown::html_vignette:
    toc: true
bibliography: lmtp.bib
vignette: >
  %\VignetteIndexEntry{getting-started}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>", 
  results = "hold"
)
require(nnls)
```

## Introduction

Most modern causal inference methods consider the effects of a treatment on a population mean outcome under interventions that set the treatment value deterministically. For example, the average treatment effect (ATE) considers the hypothetical difference in a population mean outcome if a dichotomous exposure was applied to all observations versus if it was applied to none. In the case of a continuous exposure, interventions that set the exposure to a static value deterministically are of little practical relevance. Furthermore, the estimation of causal effects requires the so called positivity assumption which states that all observations have a greater than zero chance of experiencing the exposure value under consideration. This assumption is often violated when evaluating the effects of deterministic interventions, and is usually exacerbated with longitudinal data as the number of time points grows.

Modified treatment policies (MTPs) are a class of stochastic treatment regimes that can be formulated to avoid the above problems [@munozPopulationInterventionCausal2012; @haneuseEstimationEffectInterventions2013]. In a recent article [@diazNonparametricCausalEffects2020a], we generalized the theoretical framework for estimation of the effect of MTPs to the longitudinal setting, accounting for time-varying treatment, covariates, and right-censoring of the outcome. Briefly, MTPs are hypothetical interventions where the post-intervention value of treatment can depend on the actual observed treatment level and the unit's history. As such, MTPs are useful to assess the effect of continuous treatments. For example, [@haneuseEstimationEffectInterventions2013] assess the effect of reducing surgery time by a predetermined amount (e.g., 5 minutes) for lung cancer patients, where the reduction is carried out only for those patients for whom the intervention is feasible. Furthermore, MTPs generalize many important effect estimands, such as the effect of a dynamic treatment rule in which the treatment level is assigned as a function of a unit's history. For example, dynamic treatment rules, a particular case of MTPs, may be used to estimate the effect of policies such as switching HIV treatment once the CD4 T-cell count goes below a predetermined threshold [@petersen2014delayed]. MTPs also generalize many interesting causal effects such as the average treatment effects, the causal risk ratio, and causal odds ratio. In this article we describe how **lmtp** can be used for estimating the causal effects of MTPs, and present examples on the use of the software for several of the above cases.

The package **lmtp** implements four methods for estimating the effects of MTPs. Two of these estimators, a targeted minimum-loss based estimator [@laanTargetedLearningCausal2011a; @laanTargetedMaximumLikelihood2006] and a sequentially doubly-robust [@buckleyLinearRegressionCensored1979; @fanCensoredRegressionLocal1994; @vanderlaanUnifiedCrossValidationMethodology2003; @rotnitzkyDoublyRobustEstimation2006; @rubinDoublyRobustCensoring2006; @kennedyNonparametricMethodsDoubly2017], are multiply-robust. TMLE and SDR are implemented using cross-fitting to allow for the use of flexible machine learning regression methodology [@diazNonparametricCausalEffects2020a].

### Setup

In this article, we will use the notation of [@diazNonparametricCausalEffects2020a] with only slight modifications. Let $i$ be the index of an observation from a data set with $n$ total units and $t$ be the index of time for a total number of time points $\tau$. The observed data for observation $i$ may be denoted as

$$
Z_i = (W, L_1, A_1, L_2, A_2, …, L_{\tau}, A_{\tau}, Y_{\tau + 1}) 
$$

where $W$ denotes baseline covariates, $L_t$ denotes time-varying covariates, $A_t$ denotes a vector of exposure and/or censoring variables and $Y$ denotes an outcome measured at the end of study follow-up. We observe $n$ i.i.d. copies of $Z$ with distribution $P$ . We use $A_t = a_t$ to denote a realization of a random variable. If right-censoring exists, $A_t$ can be adapted so that $A_t = (A_{1, t}, A_{2, t})$ where $A_{1, t}$ equals one if an observation is still in the study at time $t$ and zero otherwise, and $A_{2, t}$ denotes the exposure at time $t$. We use an overbar to indicate the history of a variable up until time $t$. We then use $H_t = (\bar{L}_t, \bar{A}_{t-1})$ to denote the history of all variables up until just before $A_t$.

### Modified Treatment Policies

We use the potential outcomes framework to define the causal effect of interest using our established data structure. We consider a hypothetical policy where $\bar{A}$ is set to a regime $d$ defined as $A^{d}_t = d_t(A_t, H^{d}_t)$, where $H^{d}_t = (\bar{L}_t, \bar{A}^{d}_{t-1})$, for a set of user-given regimes $d_t:t \in \{1, ..., \tau\}$. The defining characteristic that makes regime $d_t$ a modified treatment policy is that it depends on the **natural value** of treatment $\bar{A}_t$, that is, the value that the treatment would have taken under no intervention. However, when the function $d_t$ only depends on $H_t$, the LMTP reduces to the *dynamic treatment regimes* studied in the literature. Furthermore, when $d_t$ is a constant that and does not depend on either $A_t$ or $H_t$, then LMTPs reduce to the conventional static rules studied in the causal inference literature [@bang2005doubly; @van2011targeted]. Below we present examples of all these interventions.

First, consider a study of the effect of physical activity on mortality in the elderly. Assume that each patient is monitored at several time points, and that a measure of physical activity such as the metabolic equivalent of task (MET) [@mendes2018metabolic] is measured together with a number of lifestyle, health status, and demographic variables. In this setup, a natural question to ask would be "what is the effect on mortality of an intervention that increases physical activity by $\delta$ units for patients whose socioeconomic and health status allows it?'' Formally, consider a longitudinal study with loss-to-follow-up. Let $A_t = (A_{1, t}, A_{2, t})$ where$A_{1, t}$ equals one if an observation is still in the study at time $t$ and zero otherwise, and $A_{2, t}$ denote a continuous exposure at time $t$ that can be changed through some intervention. A modified treatment policy that increases $A_{2,t}$, whenever it is feasible to do so, can be defined as

$$
\begin{cases}  (1, a_{2,t} + \delta_t) & \text{if } a_{2,t} + \delta_t \leq u_t(h_t)  \\  (1, a_{2,t}) & \text{if } a_{2,t} + \delta_t > u_t(h_t)\end{cases}
$$

where $u_t(h_t)$ defines the maximum level of physical activity allowed for a patient with characteristics $h_t$. Note that we also consider an intervention on $A_{1,t}$ because we are interested in a hypothetical world where there is no loss-to-follow-up. In this case the hypothetical exposure after intervention, $A^{d}_t$ depends on the actually observed exposure, $A_t$. This is in contrast to a deterministic intervention where $A^{d}_t$ would be set to some pre-specified value with probability one.

For dynamic treatment rules, consider a hypothetical longitudinal study where two different antiviral treatments are administered to HIV positive patients. Sometimes an antiviral drug works at first, until the virus develops resistance, at which point it is necessary to change the treatment regime. Assume we are interested in assessing a policy with two treatments encoded as $A_t\in \{0,1\}$, and we want to assess the effect of a regime that would switch the antiviral treatment as soon as the CD4 T cell count drops bellow 300. Let $A_t = (A_{1, t}, A_{2, t})$ where $A_{1, t}$ equals one if an observation is still in the study at time $t$ and zero otherwise, and $A_{2, t}$ denotes the treatment arm at time $t$. Let $L_t$ denote the CD4 T cell count at time $t$. In this case, one may decide to assess the effect of the rule

$$
  d_t(h_t)=  \begin{cases}    (1, 1 - a_{2,t-1}) & \text{if } l_t < 300  \\    (1, a_{2,t-1}) & \text{if } l_t  \geq 300.  \end{cases}
$$

In contrast to the previous rule, the dynamic treatment rule does not depend on the natural value of treatment at time $t$, it only depends on the history. This induces certain technicalities in the estimation procedure for true MTPs that depend on the natural value of treatment [@diazNonparametricCausalEffects2020a]. However, the software and methods presented here handle both cases seamlessly.

In the case of a single time point setting where the data structure is $Z=(W,A,Y)$, it follows trivially from the above definitions that the average treatment effect from a cross-sectional study, defined as $\text{E}[Y(1) - Y(0)]$, can be estimated using MTPs by simply letting $\tau = 1$ and contrasting two MTPs $d(A)=1$ and $d(A)=0$. The **lmtp** package presented in this article allows the contrast of different MTPs using differences, ratios, and odds ratios.

In what follows we focus on estimating the the causal effect of MTP $d$ on outcome $Y$, using **lmtp**, through the causal parameter

$$
  \theta = \text{E}\{Y(\bar A^d)\}\text{,}
$$

where $Y(\bar A^d)$ is the counterfactual outcome in a world, where possibly contrary to fact, each entry of $\bar{A}$ was modified according to the MTP $d$. When $Y$ is continuous, $\theta$ is the mean population value of $Y$ under MTP $d$; when $Y$ is dichotomous, $\theta$ is the population proportion of event $Y$under MTP $d$. Similarly, when $Y$ is the indicator of an event by end of the study, $\theta$ is defined as the cumulative incidence of $Y$ under MTP $d$.

### Estimation methods

The **lmtp** package implements four estimation methods: a targeted minimum-loss based estimator (TMLE), a sequential doubly-robust estimator (SDR), an estimator based on the parametric G-formula, and an inverse probability weighted (IPW) estimator. We will only describe the use of TMLE, `lmtp_tmle`, and SDR, `lmtp_sdr`, as their use is strongly suggested over the others based on their advantageous theoretical properties which allow for machine learning regression while maintaining the ability to compute valid confidence intervals and p-values.

Targeted minimum-loss based estimation is a general framework for constructing asymptotically linear estimators leveraging machine learning, with an optimal bias-variance trade-off for the target causal parameter [@vanderLaanRose11; @vanderLaanRose18]. In general, TMLE is constructed from a factorization of observed data likelihood into an outcome regression and an intervention mechanism. Using the outcome regression, an initial estimate of the target parameter is constructed and then *de-biased* by a fluctuation that depends on a function of the intervention mechanism. The sequential doubly-robust estimator is based on a unbiased transformation of the efficient influence function of the target estimand.

TMLE and SDR require estimation of two nuisance parameters at each time point: an outcome mechanism and an intervention mechanism. Both TMLE and SDR are multiply-robust in that they allow certain configurations of nuisance parameters to be inconsistently estimated. Specifically, TMLE is considered $\tau + 1$-multiply robust in that it allows for inconsistent estimation of all the intervention mechanisms prior to any time point $t$, as long as all outcome mechanisms after time $t$ are consistently estimated. SDR is $2^{\tau}$-robust in that at each time point, estimation of at most either the intervention mechanism or outcome mechanism is allowed to be inconsistent. Both TMLE and SDR are efficient when all the treatment mechanism and outcome regression are consistently estimated at a given consistency rate, but the SDR has better protection against model misspecification [see @luedtke2017sequential; @rotnitzky2017multiply; @diazNonparametricCausalEffects2020a for more details].

It is important to note that the SDR estimator can produce an estimate $\hat{\theta}$ outside of the bounds of the parameter space (e.g., probability estimates outside $[0,1]$), while the TMLE guarantees that the estimate is within bounds of the parameter space. With this in mind and because for a single time-point TMLE and SDR are equally robust, we recommend use of TMLE for the case of a single time-point, while we recommend use of SDR for the longitudinal setting.

## Using LMTP

```{r}
library(lmtp)
```

### Required data structure

Data is passed to **lmtp** estimators through the `data` argument. Data should be in wide format with one column per variable per time point under study (i.e., there should be one column for every variable in $Z$). These columns do not have to be in any specific order and the data set may contain variables that are not used in estimation. The names of treatment variables, censoring variables, baseline covariates, and time-varying covariates are specified using the `trt`, `cens`, `baseline`, and `time_vary` arguments respectively. The `trt`, `cens`, and `baseline` arguments accept character vectors and the`trt` and `cens`arguments should be ordered according to the time-ordering of the data generating mechanism. The `time_vary`argument accepts an unnamed list ordered according to the time-ordering of the model with each index containing the name of the time-varying covariates for the given time. The outcome variable is specified through the `outcome` argument.

Estimators are compatible with continuous, dichotomous and survival outcomes. In the case of a dichotomous or continuous outcome, only a single variable name should be passed to the `outcome` argument. For survival outcomes, a vector containing the names of the intermediate outcome and final outcome variables, ordered according to time, should be specified with the `outcome` argument. Dichotomous and survival outcomes should be coded using zero's and one's where one indicates the occurrence of an event and zero otherwise. If working with a survival outcome, once an observation experiences an outcome, all future outcome variables should also be coded with a one. The `outcome_type` argument should be set to `"continuous"` for continuous outcomes, `"binomial"` for dichotomous, and `"survival"` for survival outcomes.

If the study is subject to loss-to-follow-up, the `cens` argument must be provided. Censoring indicators should be coded using zero's and one's where one indicates an observation is observed at the next time and zero indicates loss-to-follow-up. Once an observation's censoring status is switched to zero it cannot change back to one. Missing data before an observation is lost-to-follow-up is not allowed; a pre-processing step using multiple imputation is recommended for such variables.

The `k` argument controls a Markov assumption on the data generating mechanism. When `k = Inf`, the history $H_t$ will be constructed using all previous time-point variables while setting `k` to any other value will restrict $H_t$ to time-varying covariates from time $t - k - 1$ until $t-1$. Baseline confounders are always included in $H_t$. The `create_node_list()` function may be used to inspect how variables will be used for estimation. It is specified with the same `trt`, `baseline`, `time_vary`, and `k` arguments as **lmtp** estimators and is used internally to create a "node list'' that encodes which variables should be used at each time point of estimation. For example, consider a study with the observed data structure

$$
Z = (W_1, W_2, L_{1, 1}, L_{1, 2}, A_1, L_{2, 1}, L_{2, 2}, A_2, Y_3)
$$We can translate this data structure to R with

```{r}
baseline <- c("W_1", "W_2")
trt <- c("A_1", "A_2")
time_vary <- list(c("L_11", "L_12"), 
                  c("L_21", "L_22"))
create_node_list(trt = trt, baseline = baseline, time_vary = time_vary, tau = 2)
```

A list of lists is returned with the names of the variables in $H_t$ to be used for estimation of the outcome regression and the treatment mechanism at every time $t$.  Notice that variables $A_1$ and $A_2$ are included in the list of variables used for estimation of the treatment mechanism. This is due to the fact that the nuisance parameter for the treatment mechanism is the density ratio $r_t$, which is a function of $A_1$ and $A_2$.

The density ratio is estimated based on a classification trick using an auxiliary variable $\Lambda$ as a pseudo outcome and the treatment as a predictor. Specifically, the TMLE and SDR estimation methods require estimation of the ratio of the densities of $A_t^d$ and $A_t$, conditional on the history $H_t$, defined as $r_t$ above. This is achieved through computing the odds in a classification problem in an augmented dataset with $2n$ observations where the outcome is the auxiliary variable $\Lambda$ (defined below) and the predictors are the variables $A_t$ and $H_t$. In the $2n$ augmented data set, the data structure at time $t$ is redefined as

$$
(H_{\lambda, i, t}, A_{\lambda, i, t}, \Lambda_{\lambda, i} : \lambda = 0, 1; i = 1, ..., n)
$$

where $\Lambda_{\lambda, i} = \lambda_i$indexes duplicate values. For all duplicated observations $\lambda\in\{0,1\}$ with the same $i$, $H_{\lambda, i, t}$ is the same. For $\lambda = 0$, $A_{\lambda, i, t}$ equals the observed exposure values $A_{i, t}$, whereas for $\lambda=1$, $A_{\lambda, i, t}$ equals the exposure values under the MTP $d$, namely $A^{d}_t$. The classification approach to density ratio estimation proceeds by estimating the conditional probability that $\Delta=1$ in this dataset, and dividing it by the corresponding estimate of the conditional probability that $\Delta=0$. Specifically, denoting $P^\lambda$ the distribution of the data in the augmented dataset, we have:

$$
r_t(a_t, h_t) = \frac{p^\lambda(a_t, h_t \mid \Lambda =    1)}{p^\lambda(a_t, h_t \mid \Lambda =    0)}=\frac{P^\lambda(\Lambda = 1\mid A_t=a_t,    H_t=h_t)}{P^\lambda(\Lambda = 0\mid A_t=a_t, H_t=h_t)}.
$$

Further details on this algorithm may be found in our technical paper [@diazNonparametricCausalEffects2020a].

### Creating treatment policies

Modified treatment policies and deterministic static/dynamic treatment rules are specified using the `shift` argument, which accepts a user-defined function that returns a vector of exposure values modified according to the policy of interest. Shift functions should take two arguments, the first for specifying a data set and the second for specifying the current exposure variable. For example, a possible MTP may decrease exposure by 1 unit if the natural exposure value was greater than two and do nothing otherwise. A shift function for this MTP would look like

```{r}
shift <- function(data, trt) {
  (data[[trt]] - 1) * (data[[trt]] - 1 >= 1) + data[[trt]] * (data[[trt]] - 1 < 1)
}
```

It is then passed to estimators using the `shift` argument

```{r}
A <- c("A_1", "A_2", "A_3", "A_4")
L <- list(c("L_1"), c("L_2"), c("L_3"), c("L_4"))
lmtp_sdr(sim_t4, A, "Y", time_vary = L, k = 0, shift = shift, folds = 5)
```

This framework is flexible and allows for specifying complex treatment regimes that can depend on time and covariates. In the case of a binary exposure, two shift functions are installed with the package: `static_binary_on()` which sets $A_{i, t} = 1$, and `static_binary_off()` which sets $A_{i, t} = 0$.

```{r warning = FALSE}
if (require("twang", quietly = TRUE, attach.required = FALSE)) {
  data("iptwExWide", package = "twang")
  A <- paste0("tx", 1:3)
  W <- c("gender", "age")
  L <- list(c("use0"), c("use1"), c("use2"))
  lmtp_tmle(iptwExWide, A, "outcome", W, L, 
            shift = static_binary_on, outcome_type = "continuous",
            folds = 2, .SL_folds = 2)
}
```

Dynamic treatment regimes are treatment rules where treatment is applied based on a fixed rule that depends on covariate history. `lmtp` is capable of estimating the effects of deterministic dynamic treatment rules as well as modified treatment policies that depend on covariate history.

```{r warning = FALSE}
A <- c("A_1", "A_2", "A_3", "A_4")
L <- list(c("L_1"), c("L_2"), c("L_3"), c("L_4"))
shift <- function(data, trt) {
  (data[[trt]] - 1) * (data[[trt]] - 1 >= 1) + data[[trt]] * (data[[trt]] - 1 < 1)
}

# creating a dynamic mtp that applies the shift function 
# but also depends on history and the current time
dynamic_mtp <- function(data, trt) {
  if (trt == "A_1") {
    # if its the first time point, follow the same mtp as before
    shift(data, trt)
  } else {
    # otherwise check if the time varying covariate equals 1
    ifelse(data[[sub("A", "L", trt)]] == 1, 
           shift(data, trt), # if yes continue with the policy
           data[[trt]])      # otherwise do nothing
  }
}

lmtp_tmle(sim_t4, A, "Y", time_vary = L, k = 0, 
          shift = dynamic_mtp, folds = 2, .SL_folds = 2)
```

### SuperLearner

An attractive property of multiply-robust estimators is that they can incorporate flexible machine-learning algorithms for the estimation of nuisance parameters $Q_t$ and $r_t$ while remaining $\sqrt{n}$-consistent. The super learner algorithm is an ensemble learner than incorporates a set of candidate models through a weighted convex-combination based on cross-validation [@laanSuperLearner2007]. Asymptotically, this weighted combination of models will outperform any single one of its components.

**lmtp** uses the implementation of the super learner provided by the **SuperLearner** package [@SuperLearner]. Analysts must specify a vector of **SuperLearner** prediction algorithms which are then included in `lmtp_tmle()` and `lmtp_sdr()` calls with the `learners_trt` and `learners_outcome` arguments. The outcome variable type should guide users on selecting the appropriate candidate learners for use with the `learners_outcome` argument. Regardless of whether an exposure is continuous, dichotomous, or categorical, the exposure mechanism is estimated using classification as discussed above, users should thus only include candidate learners capable of binary classification with the `learners_trt` argument. If `learners_outcome` and `learners_trt` aren't specified, estimation will be conducted using a main-effects generalized linear model.

Candidate learners that rely on cross-validation for the tuning of hyper-parameters should support grouped data if used with `learners_trt`. Because estimation of the treatment mechanism relies on the augmented $2n$ duplicated data set, duplicated observations must be put into the same fold during sample-splitting. This is done automatically by the package.

```{r warning = FALSE}
if (require("ranger", quietly = TRUE, attach.required = FALSE) && 
    require("twang", quietly = TRUE, attach.required = FALSE)) {
  data("iptwExWide", package = "twang")
  A <- paste0("tx", 1:3)
  W <- c("gender", "age")
  L <- list(c("use0"), c("use1"), c("use2"))
  lrnrs <- c("SL.glm", "SL.ranger", "SL.glm.interaction")
  lmtp_tmle(iptwExWide, A, "outcome", W, L, shift = static_binary_on, 
            outcome_type = "continuous", learners_trt = lrnrs, 
            learners_outcome = lrnrs, folds = 2, .SL_folds = 2)
}
```

### Censored outcomes

In the case of missing outcomes, **lmtp** can estimate the effect of a hypothetical treatment regime where all observations remained uncensored at end of follow-up. To do this, the user must supply a vector containing the names of censoring indicators for each treatment time point to **lmtp** estimators through the `cens` argument. Censoring nodes should be defined such that at any time $t$, if an observation is observed at time $t + 1$ they receive a 1 and a 0 otherwise.

```{r}
head(sim_cens[sim_cens$C1 == 0, ])
```

```{r}
A <- c("A1", "A2")
L <- list(c("L1"), c("L2"))
C <- c("C1", "C2")

lmtp_tmle(sim_cens, A, "Y", time_vary = L, cens = C,
          shift = function(data, trt) data[[trt]] + 0.5, 
          folds = 2, .SL_folds = 2)
```

### Population mean outcome

In certain situations, the user may be interested in the population mean outcome under no intervention. In the presence of censoring, this can be estimated by setting `shift = NULL` and providing censoring indicators.

```{r}
A <- c("A1", "A2")
L <- list(c("L1"), c("L2"))
C <- c("C1", "C2")

lmtp_sdr(sim_cens, A, "Y", time_vary = L, cens = C, 
         shift = NULL, folds = 2, .SL_folds = 2)
```

### Survival analysis

For a time-to-event analysis, the `outcome` argument should be provided a vector containing the names of intermediate outcome variables as well as the final outcome variable; the `outcome_type` argument should be set to `"survival"`. The intermediate outcome variables serve as indicators for when an observation experiences the event before the end of follow-up. If an observation does experience the event before the final outcome time, all future outcome variables (including the final outcome) variable should be set to 1. The function `event_locf()` (last observation carried forward, only for events) is provided to help with this imputation. **Survival probability, NOT cumulative incidence, is estimated.**

Time-to-event analyses are supported for both time-invariant...

```{r warning = FALSE}
A <- "trt"
Y <- paste0("Y.", 1:6)
C <- paste0("C.", 0:5)
W <- c("W1", "W2")

lmtp_tmle(sim_point_surv, A, Y, W, cens = C, shift = static_binary_on, 
          outcome_type = "survival", folds = 2, .SL_folds = 2)
```

...and time-varying exposures.

```{r warning = FALSE}
W <- "L0.c"
L <- list(c("L0.a", "L0.b"), c("L1.a", "L1.b"))
A <- c("A0", "A1")
C <- c("C0", "C1")
Y <- c("Y1", "Y2")

lmtp_sdr(sim_timevary_surv, A, Y, W, L, C, outcome_type = "survival", 
         shift = static_binary_on, folds = 2, .SL_folds = 2)
```

### Contrasts

The effects returned by **lmtp** estimators are population intervention effects, that is the expected mean outcome in the population under the hypothetical intervention. Often, however, we are also interested in the comparison of different interventions to each other or to no intervention at all. This is the role of `lmtp_contrast()`.

```{r}
A <- c("A1", "A2")
L <- list(c("L1"), c("L2"))
C <- c("C1", "C2")

fit_shift <- 
  lmtp_sdr(sim_cens, A, "Y", time_vary = L, cens = C, 
           shift = function(data, trt) data[[trt]] + 0.5, 
           folds = 2, .SL_folds = 2)

fit_noshift <- 
  lmtp_sdr(sim_cens, A, "Y", time_vary = L, cens = C,
           shift = NULL, folds = 2, .SL_folds = 2)
```

```{r}
lmtp_contrast(fit_shift, ref = fit_noshift, type = "additive")
```

```{r}
lmtp_contrast(fit_shift, ref = fit_noshift, type = "rr")
```

### Extra features

##### Tidy results

**lmtp** provides a `tidy()` method as described in **broom** [@broom]:

```{r}
tidy(fit_shift)
```

##### Parallel processing

Computation time can quickly increase in with many time-points, a large Super Learner library, and large datasets. To help, **lmtp** provides support for parallel processing using **future** [@future]. The simplest way to use estimators in parallel is to run `plan(multiprocess)`. We recommend consulting the **future** [documentation](https://cran.r-project.org/package=future) for more information.

##### Progress bars

In the presence of long computation time, a lack of user feedback can become very frustrating. To address this, **lmtp** supports the use of progress bars during computation through **progressr** [@progressr].

## References
